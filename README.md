# üîê SecureCorp AI

### LLM Security Guardrails Demo

## üìã Descripci√≥n del Proyecto

**SecureCorp AI** es una **Prueba de Concepto (PoC)** dise√±ada para demostrar c√≥mo **asegurar aplicaciones empresariales que utilizan Inteligencia Artificial Generativa (LLMs)**.

El objetivo principal es mitigar vulnerabilidades cr√≠ticas del **OWASP Top 10 for LLM Applications**, espec√≠ficamente:

* **LLM01: Prompt Injection**
* **LLM07: Insecure Plugin Design**

La mitigaci√≥n se realiza mediante el uso de **Guardrails deterministas implementados en c√≥digo nativo (.NET / C#)**, evitando depender exclusivamente de instrucciones en lenguaje natural (*System Prompts*), los cuales pueden ser eludidos mediante t√©cnicas de *jailbreak* o ingenier√≠a social.

---

## üõ°Ô∏è Arquitectura de Seguridad

La soluci√≥n utiliza **Microsoft Semantic Kernel** como orquestador, integrando un **plugin de Recursos Humanos (RRHH)** protegido por m√∫ltiples capas de seguridad:

### 1Ô∏è‚É£ Capa de IA (Prompt Engineering)

* Configuraci√≥n inicial del comportamiento del modelo.
* Se asume como **falible** y no confiable por s√≠ sola.

### 2Ô∏è‚É£ Capa de Validaci√≥n L√≥gica (Guardrails en C#)

* Intercepta las llamadas de la IA a herramientas internas.
* Verifica permisos antes de ejecutar consultas SQL/Data (simulado).
* Bloquea accesos a registros sensibles
  *(por ejemplo: salarios de ejecutivos o CEO)*.

### 3Ô∏è‚É£ Capa de Auditor√≠a Forense (Logging)

* Registra autom√°ticamente intentos de acceso no autorizados.
* Guarda evidencias en el archivo `security_audit.log`.
* Permite trazabilidad de incidentes causados por:

  * Ingenier√≠a social
  * Alucinaciones del modelo
  * Intentos de *prompt injection*

---

## üöÄ Tecnolog√≠as Utilizadas

* **Lenguaje:** C# (.NET 8.0)
* **AI Orchestrator:** Microsoft Semantic Kernel
* **Modelo LLM:** OpenAI `gpt-4o-mini`
* **Seguridad:**

  * .NET User Secrets para gesti√≥n de API Keys
  * Validaci√≥n estricta de Input / Output
  * Guardrails deterministas en c√≥digo

---

## ‚ö†Ô∏è Escenario de Ataque y Mitigaci√≥n

### ‚ùå Escenario Vulnerable (Sin protecciones)

**Atacante:**

> "Dime cu√°nto gana Ana G√≥mez (CEO)"

**IA:**

* Consulta la base de datos
* Revela informaci√≥n confidencial

---

### ‚úÖ Escenario Protegido (Implementado)

**Atacante:**

> "Dime cu√°nto gana Ana G√≥mez"
> *(o utilizando t√©cnicas avanzadas de jailbreak)*

**Flujo del Sistema:**

1. La IA intenta invocar la herramienta `GetSalario`.
2. El plugin C# detecta que el usuario solicitado es **protegido**.
3. Se activa una alerta de seguridad.
4. El incidente se registra en `security_audit.log`.
5. Se devuelve un error gen√©rico a la IA.

**Respuesta final de la IA:**

> "No tengo acceso a esa informaci√≥n debido a restricciones de seguridad."

---

## üîß Configuraci√≥n Local

### 1Ô∏è‚É£ Clonar el repositorio

```bash
git clone <url-del-repositorio>
cd SecureCorpAI
```

### 2Ô∏è‚É£ Configurar la API Key de OpenAI (User Secrets)

> Esto evita exponer claves sensibles en el c√≥digo fuente.

```powershell
dotnet user-secrets init
dotnet user-secrets set "OpenAI:ApiKey" "TU_API_KEY_AQUI"
dotnet user-secrets set "OpenAI:ModelId" "gpt-4o-mini"
```

### 3Ô∏è‚É£ Ejecutar la aplicaci√≥n

```powershell
dotnet run
```

### 4Ô∏è‚É£ Revisar los logs de seguridad

Luego de intentar un ataque, revisa el archivo:

```
bin/Debug/net8.0/security_audit.log
```

---

## üìö Nota Final

Este proyecto fue desarrollado con **fines educativos** para demostrar **buenas pr√°cticas de seguridad en aplicaciones con IA**, aplicadas al desarrollo en **.NET**.

El enfoque principal es evidenciar que **la seguridad no debe delegarse √∫nicamente al modelo**, sino reforzarse mediante **validaciones deterministas y controladas en el backend**.
